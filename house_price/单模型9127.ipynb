{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"data/train_data.csv\")\n",
    "test=pd.read_csv(\"data/test_a.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=train[train[\"tradeMoney\"]<25000]\n",
    "# train=train[train[\"area\"]<200]\n",
    "# train=train[train[\"area\"]>12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traiin=train[(train[\"tradeMoney\"]<18000)&(train[\"tradeMoney\"]>100) ]\n",
    "\n",
    "# train=train[train[\"area\"]<200]\n",
    "# train=train[train[\"area\"]>10]\n",
    "\n",
    "# train = train [train['totalFloor'] <80]\n",
    "# train = train [train['tradeMeanPrice'] <100000]\n",
    "# train = train [train['remainNewNum'] <4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['tradeMoney']=-1\n",
    "data=pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rentType\n",
      "houseType\n",
      "houseFloor\n",
      "houseToward\n",
      "houseDecoration\n",
      "communityName\n",
      "city\n",
      "region\n",
      "plate\n",
      "buildYear\n",
      "tradeTime\n"
     ]
    }
   ],
   "source": [
    "for col in data.columns:\n",
    "    if data[col].dtype==\"object\":\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_local(df):\n",
    "    return df['region']+\"_\"+df['plate']+\"_\"+df['communityName']\n",
    "data['local']=data.apply(lambda x:get_local(x),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['houseToward']=data['houseToward'].apply(lambda x:x.replace(\"暂无数据\",\"南\"),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def datetime_timestamp(dt):\n",
    "    s = time.mktime(time.strptime(dt,'%Y/%m/%d'))\n",
    "    return s\n",
    "def get_feat(data):\n",
    "    data['pv']=data['pv'].fillna(data['pv'].mean())\n",
    "    data['uv']=data['uv'].fillna(data['uv'].mean())\n",
    "    data['rentType']=data['rentType'].map({\"整租\":1,\"合租\":2})\n",
    "    data['room_num']=data['houseType'].apply(lambda x:int(x[0]),1)\n",
    "    data['hall_num']=data['houseType'].apply(lambda x:int(x[2]),1)\n",
    "    data['toilet_num']=data['houseType'].apply(lambda x:int(x[4]),1)\n",
    "    data['total_num']=data['room_num']+data['toilet_num']+data['hall_num']\n",
    "    data['houseFloor']=data['houseFloor'].map({\"低\":0,\"中\":1,\"高\":2})\n",
    "    data['houseDecoration']=data['houseDecoration'].map({\"其他\":0,\"毛坯\":1,\"简装\":2,\"精装\":3})\n",
    "    data['communityName_freq']=data['communityName'].map(data['communityName'].value_counts().rank()/len(data['communityName'].unique()))\n",
    "    data['region']=data['region'].map(data['region'].value_counts().rank()/len(data['region'].unique()))\n",
    "    data['plate']=data['plate'].map(data['plate'].value_counts().rank()/len(data['plate'].unique()))\n",
    "    data['local']=data['local'].map(data['local'].value_counts().rank()/len(data['local'].unique()))\n",
    "#     data['houseType']=data['houseType'].map(data['houseType'].value_counts().rank()/len(data['houseType'].unique()))\n",
    "    data['buildYear']=data['buildYear'].apply(lambda x:x.replace(\"暂无信息\",\"1970\"),1)\n",
    "    data['buildYear']=data['buildYear'].astype(int)\n",
    "    data['year']=data['tradeTime'].apply(lambda x:int(x.split(\"/\")[0]),1)\n",
    "    data['month']=data['tradeTime'].apply(lambda x:int(x.split(\"/\")[1]),1)\n",
    "    data['day']=data['tradeTime'].apply(lambda x:int(x.split(\"/\")[2]),1)\n",
    "#     data['build_time']=data['year']-data['buildYear']\n",
    "    data['tradeTime']=data['tradeTime'].apply(lambda x:datetime_timestamp(x),1)\n",
    "    for col in ['houseType',\"houseToward\",\"communityName\",]:\n",
    "        lbl =LabelEncoder()\n",
    "        data[col]=lbl.fit_transform(data[col])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=get_feat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['room_all'] = data['room_num'] + data['hall_num'] +data['toilet_num'] \n",
    "    \n",
    "data['per_room'] = data['area'] / data['room_all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['avg_money']=data['tradeMoney']/data['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data[data.tradeMoney!=-1]\n",
    "test=data[data.tradeMoney==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "train['per_area'] =  train['tradeMoney']/train['area']\n",
    "train=train[(train[\"per_area\"]<1000)&(train[\"per_area\"]>25) ]\n",
    "\n",
    "train=train[(train[\"tradeMoney\"]<18000)&(train[\"tradeMoney\"]>100) ]\n",
    "\n",
    "train=train[train[\"area\"]<200]\n",
    "train=train[train[\"area\"]>10]\n",
    "\n",
    "train = train [train['totalFloor'] <80]\n",
    "train = train [train['tradeMeanPrice'] <100000]\n",
    "train = train [train['remainNewNum'] <4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x213402db358>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "get_ipython().magic('matplotlib inline')\n",
    "sns.distplot(train['houseToward'].fillna(0),color=\"blue\")\n",
    "sns.distplot(test['houseToward'].fillna(0),color=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from itertools import product\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "def get_mode(row):\n",
    "    return stats.mode(row)[0][0]  \n",
    "def get_unique_num(row):\n",
    "    if len(row) == 0:\n",
    "        return 0\n",
    "    return len(np.unique(row))\n",
    "def get_len(row):\n",
    "    return len(row)\n",
    "def get_unique_freq(row):\n",
    "    if len(row) == 0:\n",
    "        return 0\n",
    "    return float(len(np.unique(row))) / len(row)\n",
    "def get_max(row):\n",
    "    return np.max(row)\n",
    "def get_min(row):\n",
    "    return np.min(row)\n",
    "def get_mean(row):\n",
    "    return np.mean(row)\n",
    "def get_std(row):\n",
    "    return np.std(row)\n",
    "def get_skew(row):\n",
    "    return skew(row)\n",
    "class MeanEncoder:\n",
    "    def __init__(self, categorical_features,stats=\"mean\", flag=\"mean\",n_splits=5, target_type='classification', prior_weight_func=None):\n",
    "        \"\"\"\n",
    "        :param categorical_features: list of str, the name of the categorical columns to encode\n",
    "        :param n_splits: the number of splits used in mean encoding\n",
    "        :param target_type: str, 'regression' or 'classification'\n",
    "        :param prior_weight_func:\n",
    "        a function that takes in the number of observations, and outputs prior weight\n",
    "        when a dict is passed, the default exponential decay function will be used:\n",
    "        k: the number of observations needed for the posterior to be weighted equally as the prior\n",
    "        f: larger f --> smaller slope\n",
    "        '''\n",
    "        >>>example:\n",
    "        mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['regionidcity',\n",
    "                          'regionidneighborhood', 'regionidzip'],\n",
    "                target_type='regression'\n",
    "                )\n",
    "\n",
    "        X = mean_encoder.fit_transform(X, pd.Series(y))\n",
    "        X_test = mean_encoder.transform(X_test)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.categorical_features = categorical_features\n",
    "        self.n_splits = n_splits\n",
    "        self.learned_stats = {}\n",
    "        self.stats=stats\n",
    "        self.flag=flag\n",
    "        if target_type == 'classification':\n",
    "            self.target_type = target_type\n",
    "            self.target_values = []\n",
    "        else:\n",
    "            self.target_type = 'regression'\n",
    "            self.target_values = None\n",
    "\n",
    "        if isinstance(prior_weight_func, dict):\n",
    "            self.prior_weight_func = eval('lambda x: 1 / (1 + np.exp((x - k) / f))', dict(prior_weight_func, np=np))\n",
    "        elif callable(prior_weight_func):\n",
    "            self.prior_weight_func = prior_weight_func\n",
    "        else:\n",
    "            self.prior_weight_func = lambda x: 1 / (1 + np.exp((x - 2) / 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func,stats,flag):\n",
    "        X_train = X_train[[variable]].copy()\n",
    "        X_test = X_test[[variable]].copy()\n",
    "       \n",
    "        if target is not None:\n",
    "            nf_name = '{}_pred_{}_{}'.format(variable, target,stats)\n",
    "            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n",
    "        else:\n",
    "            nf_name = '{}_pred_{}'.format(variable,stats)\n",
    "            X_train['pred_temp'] = y_train  # regression\n",
    "#         prior = X_train['pred_temp'].mean()\n",
    "        stats_dict={\"mean\":get_mean,\n",
    "                    \"min\":get_min,\n",
    "                    \"max\":get_max,\n",
    "                    \"std\":get_std,\n",
    "                    \"skew\":get_skew,\n",
    "                    \"mode\":get_mode,\n",
    "                    \"kurt\":kurtosis,\n",
    "                    \"unique\":get_unique_num,\n",
    "                    \"freq\":get_unique_freq\n",
    "                   \n",
    "                   } \n",
    "        if stats==\"mode\":\n",
    "            col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({stats: get_mode, 'beta': 'size'})\n",
    "        if stats==\"kurt\":\n",
    "            col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({stats: kurtosis, 'beta': 'size'})\n",
    "        if stats==\"unique\":\n",
    "            prior = get_unique_num(X_train['pred_temp'])\n",
    "            col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({stats: get_unique_num, 'beta': 'size'})\n",
    "        if stats==\"freq\":\n",
    "            prior = get_unique_freq(X_train['pred_temp'])\n",
    "            col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({stats: get_unique_freq, 'beta': 'size'})\n",
    "        \n",
    "        if stats in [\"mean\",\"max\",\"min\",\"sum\",\"std\",\"var\",\"median\",\"skew\"]: \n",
    "            col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({stats: stats, 'beta': 'size'})\n",
    "            \n",
    "        if flag==\"mean\":\n",
    "            prior = stats_dict['mean'](X_train['pred_temp'])\n",
    "        elif flag==\"median\":\n",
    "            prior = X_train['pred_temp'].median()\n",
    "        else:\n",
    "            prior = stats_dict[stats](X_train['pred_temp'])\n",
    "        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n",
    "        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y[stats]\n",
    "        col_avg_y.drop(['beta', stats], axis=1, inplace=True)\n",
    "\n",
    "        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n",
    "        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n",
    "\n",
    "        return nf_train, nf_test, prior, col_avg_y\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :param y: pandas Series or numpy array, n_samples\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        stats=self.stats\n",
    "        if self.target_type == 'classification':\n",
    "            skf = StratifiedKFold(self.n_splits)\n",
    "        else:\n",
    "            skf = KFold(self.n_splits)\n",
    "\n",
    "        if self.target_type == 'classification':\n",
    "            self.target_values = sorted(set(y))\n",
    "            self.learned_stats = {'{}_pred_{}_{}'.format(variable, target,stats): [] for variable, target in\n",
    "                                  product(self.categorical_features, self.target_values)}\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}_{}'.format(variable, target,stats)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func,self.stats,self.flag)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        else:\n",
    "            self.learned_stats = {'{}_pred_{}'.format(variable,stats): [] for variable in self.categorical_features}\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred_{}'.format(variable,stats)\n",
    "                X_new.loc[:, nf_name] = np.nan\n",
    "                for large_ind, small_ind in skf.split(y, y):\n",
    "                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n",
    "                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func,self.stats,self.flag)\n",
    "                    X_new.iloc[small_ind, -1] = nf_small\n",
    "                    self.learned_stats[nf_name].append((prior, col_avg_y))\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        :param X: pandas DataFrame, n_samples * n_features\n",
    "        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n",
    "        \"\"\"\n",
    "        X_new = X.copy()\n",
    "        stats=self.stats\n",
    "        if self.target_type == 'classification':\n",
    "            for variable, target in product(self.categorical_features, self.target_values):\n",
    "                nf_name = '{}_pred_{}_{}'.format(variable, target,stats)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "        else:\n",
    "            for variable in self.categorical_features:\n",
    "                nf_name = '{}_pred_{}'.format(variable,stats)\n",
    "                X_new[nf_name] = 0\n",
    "                for prior, col_avg_y in self.learned_stats[nf_name]:\n",
    "                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n",
    "                        nf_name]\n",
    "                X_new[nf_name] /= self.n_splits\n",
    "\n",
    "        return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36910"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traiin=train[(train[\"tradeMoney\"]<18000)&(train[\"tradeMoney\"]>100) ]\n",
    "\n",
    "# train=train[train[\"area\"]<200]\n",
    "# train=train[train[\"area\"]>10]\n",
    "\n",
    "# train = train [train['totalFloor'] <80]\n",
    "# train = train [train['tradeMeanPrice'] <100000]\n",
    "# train = train [train['remainNewNum'] <4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train=train[train[\"tradeMoney\"]<15000]\n",
    "# train=train[train[\"area\"]<200]\n",
    "# train=train[train[\"area\"]>12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.index=range(len(train))\n",
    "test.index=range(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:108: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std...\n",
      "max...\n",
      "min...\n",
      "skew...\n",
      "kurt...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:99: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:102: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freq...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:105: FutureWarning: using a dict on a Series for aggregation\n",
      "is deprecated and will be removed in a future version\n"
     ]
    }
   ],
   "source": [
    "flag=\"mean\"\n",
    "print(\"mean...\")\n",
    "mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['communityName',\"plate\",\"buildYear\",\"totalFloor\",\"day\"],\n",
    "                target_type='regression',flag=flag\n",
    "                )\n",
    "\n",
    "train = mean_encoder.fit_transform(train, pd.Series(train['tradeMoney']))\n",
    "test = mean_encoder.transform(test)\n",
    "print(\"std...\")\n",
    "mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['communityName',\"plate\",\"buildYear\",\"totalFloor\",\"day\"],\n",
    "                stats=\"std\",\n",
    "                target_type='regression',flag=flag\n",
    "                )\n",
    "\n",
    "train = mean_encoder.fit_transform(train, pd.Series(train['tradeMoney']))\n",
    "test = mean_encoder.transform(test)\n",
    "\n",
    "print(\"max...\")\n",
    "mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['communityName',\"plate\",\"buildYear\",\"totalFloor\",\"day\"],\n",
    "                stats=\"max\",\n",
    "                target_type='regression',flag=flag\n",
    "                )\n",
    "\n",
    "train = mean_encoder.fit_transform(train, pd.Series(train['tradeMoney']))\n",
    "test = mean_encoder.transform(test)\n",
    "print(\"min...\")\n",
    "mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['communityName',\"plate\",\"buildYear\",\"totalFloor\",\"day\"],\n",
    "                stats=\"min\",\n",
    "                target_type='regression',flag=flag\n",
    "                )\n",
    "\n",
    "train = mean_encoder.fit_transform(train, pd.Series(train['tradeMoney']))\n",
    "test = mean_encoder.transform(test)\n",
    "print(\"skew...\")\n",
    "mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['communityName',\"plate\",\"buildYear\",\"totalFloor\",\"day\"],\n",
    "                stats=\"skew\",\n",
    "                target_type='regression',flag=flag\n",
    "                )\n",
    "\n",
    "train = mean_encoder.fit_transform(train, pd.Series(train['tradeMoney']))\n",
    "test = mean_encoder.transform(test)\n",
    "print(\"kurt...\")\n",
    "mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['communityName',\"plate\",\"buildYear\",\"totalFloor\",\"day\"],\n",
    "                stats=\"kurt\",\n",
    "                target_type='regression',flag=flag\n",
    "                )\n",
    "\n",
    "train = mean_encoder.fit_transform(train, pd.Series(train['tradeMoney']))\n",
    "test = mean_encoder.transform(test)\n",
    "print(\"unique...\")\n",
    "mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['communityName',\"plate\",\"buildYear\",\"totalFloor\",\"day\"],\n",
    "                stats=\"unique\",\n",
    "                target_type='regression',flag=flag\n",
    "                )\n",
    "\n",
    "train = mean_encoder.fit_transform(train, pd.Series(train['tradeMoney']))\n",
    "test = mean_encoder.transform(test)\n",
    "print(\"freq...\")\n",
    "mean_encoder = MeanEncoder(\n",
    "                        categorical_features=['communityName',\"plate\",\"buildYear\",\"totalFloor\",\"day\"],\n",
    "                stats=\"freq\",\n",
    "                target_type='regression',flag=flag\n",
    "                )\n",
    "\n",
    "train = mean_encoder.fit_transform(train, pd.Series(train['tradeMoney']))\n",
    "test = mean_encoder.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def substract(x, y):\n",
    "    return x - y\n",
    "\n",
    "\n",
    "def times(x, y):\n",
    "    return x * y\n",
    "\n",
    "\n",
    "def divide(x, y):\n",
    "    return (x + 0.001)/(y + 0.001)\n",
    "\n",
    "\n",
    "CrossMethod = {\n",
    "#                     '+':add,\n",
    "#                    '-':substract,\n",
    "    '*': times,\n",
    "    '/': divide, }\n",
    "\n",
    "\n",
    "def get_cv_feat(cv_col, train):\n",
    "    bh = pd.DataFrame()\n",
    "\n",
    "    for i in range(len(cv_col)):\n",
    "        for j in range(i+1, len(cv_col)):\n",
    "            for k in CrossMethod:\n",
    "                bh[cv_col[i]+k+cv_col[j]\n",
    "                   ] = CrossMethod[k](train[cv_col[i]], train[cv_col[j]])\n",
    "#                 print(cv_col[i]+k+cv_col[j],\"done\")\n",
    "    return bh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_train=get_cv_feat(f15,train)\n",
    "# cv_test=get_cv_feat(f15,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=train#.join(cv_train)\n",
    "test=test#.join(cv_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data=pd.concat([train,test])\n",
    "groupby_feat=data.groupby(\"communityName\",as_index=False)['area'].agg({\"area_mean\":\"mean\",\"area_std\":\"std\",\"area_skew\":\"skew\",\n",
    "                                                         \"area_min\":\"min\",\"area_max\":\"max\",})\n",
    "\n",
    "groupby_feat1=data.groupby(\"communityName\",as_index=False)['day'].agg({\"day_mean\":\"mean\",\"day_std\":\"std\",\"day_skew\":\"skew\",\n",
    "                                                         \"day_min\":\"min\",\"day_max\":\"max\",})\n",
    "\n",
    "groupby_feat2=data.groupby(\"communityName\",as_index=False)['month'].agg({\"month_mean\":\"mean\",\"month_std\":\"std\",\"month_skew\":\"skew\",\n",
    "                                                         \"month_min\":\"min\",\"month_max\":\"max\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.merge(train,groupby_feat,on=\"communityName\",how=\"left\")\n",
    "test=pd.merge(test,groupby_feat,on=\"communityName\",how=\"left\")\n",
    "\n",
    "train=pd.merge(train,groupby_feat1,on=\"communityName\",how=\"left\")\n",
    "test=pd.merge(test,groupby_feat1,on=\"communityName\",how=\"left\")\n",
    "\n",
    "train=pd.merge(train,groupby_feat2,on=\"communityName\",how=\"left\")\n",
    "test=pd.merge(test,groupby_feat2,on=\"communityName\",how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['area-mean']=train['area']-train['area_mean']\n",
    "# train['area-mode']=train['area']-train['area_mode']\n",
    "# train['area-mean/std']=train['area-mean']/train['area_std']\n",
    "# train['area-mode/std']=train['area-mode']/train['area_std']\n",
    "# train['area-mean/max-min']=train['area-mean']/(train['area_max']-train['area_min'])\n",
    "\n",
    "# test['area-mean']=test['area']-test['area_mean']\n",
    "# test['area-mode']=test['area']-test['area_mode']\n",
    "# test['area-mean/std']=test['area-mean']/test['area_std']\n",
    "# test['area-mode/std']=test['area-mode']/test['area_std']\n",
    "# test['area-mean/max-min']=test['area-mean']/(test['area_max']-test['area_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [x for x in train.columns if x not in [\"per_area\",\"avg_money\",\"communityName\",'ID',\"year\",\"city\",\"tradeMoney\",'tradeTime','total_num']]\n",
    "label = \"tradeMoney\"\n",
    "# label = \"avg_money\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y_pred,train_data):\n",
    "    y_true=train_data.get_label()\n",
    "    return \"r2\",r2_score(y_true,y_pred),True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGB | FOLD 1/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1013.8\ttraining's r2: 0.843417\tvalid_1's rmse: 1059.13\tvalid_1's r2: 0.83704\n",
      "[400]\ttraining's rmse: 817.352\ttraining's r2: 0.898222\tvalid_1's rmse: 880.832\tvalid_1's r2: 0.887288\n",
      "[600]\ttraining's rmse: 757.65\ttraining's r2: 0.912547\tvalid_1's rmse: 838.193\tvalid_1's r2: 0.897937\n",
      "[800]\ttraining's rmse: 722.683\ttraining's r2: 0.920433\tvalid_1's rmse: 816.537\tvalid_1's r2: 0.903142\n",
      "[1000]\ttraining's rmse: 697.491\ttraining's r2: 0.925884\tvalid_1's rmse: 802.15\tvalid_1's r2: 0.906525\n",
      "[1200]\ttraining's rmse: 677.576\ttraining's r2: 0.930056\tvalid_1's rmse: 792.306\tvalid_1's r2: 0.908805\n",
      "[1400]\ttraining's rmse: 661.312\ttraining's r2: 0.933373\tvalid_1's rmse: 784.917\tvalid_1's r2: 0.910499\n",
      "[1600]\ttraining's rmse: 647.247\ttraining's r2: 0.936177\tvalid_1's rmse: 779.197\tvalid_1's r2: 0.911798\n",
      "[1800]\ttraining's rmse: 635.169\ttraining's r2: 0.938537\tvalid_1's rmse: 774.657\tvalid_1's r2: 0.912823\n",
      "[2000]\ttraining's rmse: 624.447\ttraining's r2: 0.940594\tvalid_1's rmse: 771.127\tvalid_1's r2: 0.913616\n",
      "[2200]\ttraining's rmse: 614.505\ttraining's r2: 0.942471\tvalid_1's rmse: 768.289\tvalid_1's r2: 0.91425\n",
      "[2400]\ttraining's rmse: 605.678\ttraining's r2: 0.944112\tvalid_1's rmse: 765.37\tvalid_1's r2: 0.914901\n",
      "[2600]\ttraining's rmse: 597.052\ttraining's r2: 0.945692\tvalid_1's rmse: 763.128\tvalid_1's r2: 0.915399\n",
      "[2800]\ttraining's rmse: 589.086\ttraining's r2: 0.947132\tvalid_1's rmse: 760.918\tvalid_1's r2: 0.915888\n",
      "[3000]\ttraining's rmse: 581.679\ttraining's r2: 0.948453\tvalid_1's rmse: 759.018\tvalid_1's r2: 0.916307\n",
      "[3200]\ttraining's rmse: 574.507\ttraining's r2: 0.949716\tvalid_1's rmse: 757.485\tvalid_1's r2: 0.916645\n",
      "[3400]\ttraining's rmse: 568.09\ttraining's r2: 0.950833\tvalid_1's rmse: 756.258\tvalid_1's r2: 0.916915\n",
      "[3600]\ttraining's rmse: 561.73\ttraining's r2: 0.951928\tvalid_1's rmse: 754.866\tvalid_1's r2: 0.917221\n",
      "[3800]\ttraining's rmse: 555.703\ttraining's r2: 0.952954\tvalid_1's rmse: 753.617\tvalid_1's r2: 0.917494\n",
      "[4000]\ttraining's rmse: 549.798\ttraining's r2: 0.953949\tvalid_1's rmse: 752.42\tvalid_1's r2: 0.917756\n",
      "[4200]\ttraining's rmse: 544.168\ttraining's r2: 0.954887\tvalid_1's rmse: 751.647\tvalid_1's r2: 0.917925\n",
      "[4400]\ttraining's rmse: 538.537\ttraining's r2: 0.955816\tvalid_1's rmse: 750.774\tvalid_1's r2: 0.918116\n",
      "[4600]\ttraining's rmse: 533.249\ttraining's r2: 0.956679\tvalid_1's rmse: 750.086\tvalid_1's r2: 0.918266\n",
      "[4800]\ttraining's rmse: 528.203\ttraining's r2: 0.957495\tvalid_1's rmse: 749.347\tvalid_1's r2: 0.918427\n",
      "[5000]\ttraining's rmse: 523.109\ttraining's r2: 0.958311\tvalid_1's rmse: 748.817\tvalid_1's r2: 0.918542\n",
      "[5200]\ttraining's rmse: 518.41\ttraining's r2: 0.959057\tvalid_1's rmse: 748.281\tvalid_1's r2: 0.918659\n",
      "[5400]\ttraining's rmse: 513.683\ttraining's r2: 0.9598\tvalid_1's rmse: 747.789\tvalid_1's r2: 0.918766\n",
      "[5600]\ttraining's rmse: 509.271\ttraining's r2: 0.960487\tvalid_1's rmse: 747.388\tvalid_1's r2: 0.918853\n",
      "[5800]\ttraining's rmse: 504.945\ttraining's r2: 0.961156\tvalid_1's rmse: 746.921\tvalid_1's r2: 0.918954\n",
      "[6000]\ttraining's rmse: 500.698\ttraining's r2: 0.961807\tvalid_1's rmse: 746.426\tvalid_1's r2: 0.919061\n",
      "[6200]\ttraining's rmse: 496.391\ttraining's r2: 0.962461\tvalid_1's rmse: 746.011\tvalid_1's r2: 0.919151\n",
      "[6400]\ttraining's rmse: 492.305\ttraining's r2: 0.963076\tvalid_1's rmse: 745.587\tvalid_1's r2: 0.919243\n",
      "[6600]\ttraining's rmse: 488.301\ttraining's r2: 0.963674\tvalid_1's rmse: 745.224\tvalid_1's r2: 0.919322\n",
      "[6800]\ttraining's rmse: 484.408\ttraining's r2: 0.964251\tvalid_1's rmse: 744.91\tvalid_1's r2: 0.91939\n",
      "[7000]\ttraining's rmse: 480.685\ttraining's r2: 0.964799\tvalid_1's rmse: 744.822\tvalid_1's r2: 0.919409\n",
      "Early stopping, best iteration is:\n",
      "[6879]\ttraining's rmse: 482.941\ttraining's r2: 0.964468\tvalid_1's rmse: 744.758\tvalid_1's r2: 0.919423\n",
      "Predict 1/2\n",
      "LGB cv score 1: LOSS 0.9194227263420797 \n",
      "################################################################################\n",
      "LGB | FOLD 2/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1020.73\ttraining's r2: 0.842944\tvalid_1's rmse: 1021.12\tvalid_1's r2: 0.842038\n",
      "[400]\ttraining's rmse: 825.622\ttraining's r2: 0.897247\tvalid_1's rmse: 844.654\tvalid_1's r2: 0.891917\n",
      "[600]\ttraining's rmse: 765.78\ttraining's r2: 0.911602\tvalid_1's rmse: 799.742\tvalid_1's r2: 0.903105\n",
      "[800]\ttraining's rmse: 730.381\ttraining's r2: 0.919586\tvalid_1's rmse: 776.253\tvalid_1's r2: 0.908713\n",
      "[1000]\ttraining's rmse: 704.833\ttraining's r2: 0.925113\tvalid_1's rmse: 762.136\tvalid_1's r2: 0.912003\n",
      "[1200]\ttraining's rmse: 684.954\ttraining's r2: 0.929278\tvalid_1's rmse: 752.384\tvalid_1's r2: 0.914241\n",
      "[1400]\ttraining's rmse: 668.685\ttraining's r2: 0.932597\tvalid_1's rmse: 744.808\tvalid_1's r2: 0.915959\n",
      "[1600]\ttraining's rmse: 654.605\ttraining's r2: 0.935406\tvalid_1's rmse: 738.658\tvalid_1's r2: 0.917341\n",
      "[1800]\ttraining's rmse: 642.335\ttraining's r2: 0.937805\tvalid_1's rmse: 733.838\tvalid_1's r2: 0.918417\n",
      "[2000]\ttraining's rmse: 631.586\ttraining's r2: 0.939869\tvalid_1's rmse: 730.032\tvalid_1's r2: 0.919261\n",
      "[2200]\ttraining's rmse: 621.645\ttraining's r2: 0.941747\tvalid_1's rmse: 726.215\tvalid_1's r2: 0.920103\n",
      "[2400]\ttraining's rmse: 612.423\ttraining's r2: 0.943462\tvalid_1's rmse: 723.652\tvalid_1's r2: 0.920666\n",
      "[2600]\ttraining's rmse: 603.781\ttraining's r2: 0.945047\tvalid_1's rmse: 720.916\tvalid_1's r2: 0.921265\n",
      "[2800]\ttraining's rmse: 595.935\ttraining's r2: 0.946466\tvalid_1's rmse: 718.84\tvalid_1's r2: 0.921717\n",
      "[3000]\ttraining's rmse: 588.316\ttraining's r2: 0.947826\tvalid_1's rmse: 717.019\tvalid_1's r2: 0.922113\n",
      "[3200]\ttraining's rmse: 581.309\ttraining's r2: 0.949061\tvalid_1's rmse: 715.36\tvalid_1's r2: 0.922473\n",
      "[3400]\ttraining's rmse: 574.485\ttraining's r2: 0.95025\tvalid_1's rmse: 714.052\tvalid_1's r2: 0.922757\n",
      "[3600]\ttraining's rmse: 568.015\ttraining's r2: 0.951365\tvalid_1's rmse: 712.588\tvalid_1's r2: 0.923073\n",
      "[3800]\ttraining's rmse: 561.923\ttraining's r2: 0.952402\tvalid_1's rmse: 711.319\tvalid_1's r2: 0.923347\n",
      "[4000]\ttraining's rmse: 556.073\ttraining's r2: 0.953388\tvalid_1's rmse: 710.272\tvalid_1's r2: 0.923572\n",
      "[4200]\ttraining's rmse: 550.295\ttraining's r2: 0.954352\tvalid_1's rmse: 709.551\tvalid_1's r2: 0.923728\n",
      "[4400]\ttraining's rmse: 544.935\ttraining's r2: 0.955237\tvalid_1's rmse: 708.649\tvalid_1's r2: 0.923921\n",
      "[4600]\ttraining's rmse: 539.346\ttraining's r2: 0.95615\tvalid_1's rmse: 707.63\tvalid_1's r2: 0.92414\n",
      "[4800]\ttraining's rmse: 533.962\ttraining's r2: 0.957021\tvalid_1's rmse: 706.702\tvalid_1's r2: 0.924339\n",
      "[5000]\ttraining's rmse: 528.767\ttraining's r2: 0.957854\tvalid_1's rmse: 705.86\tvalid_1's r2: 0.924519\n",
      "[5200]\ttraining's rmse: 523.796\ttraining's r2: 0.958642\tvalid_1's rmse: 704.985\tvalid_1's r2: 0.924706\n",
      "[5400]\ttraining's rmse: 519.077\ttraining's r2: 0.959384\tvalid_1's rmse: 704.392\tvalid_1's r2: 0.924833\n",
      "[5600]\ttraining's rmse: 514.479\ttraining's r2: 0.9601\tvalid_1's rmse: 703.456\tvalid_1's r2: 0.925032\n",
      "[5800]\ttraining's rmse: 510.134\ttraining's r2: 0.960772\tvalid_1's rmse: 702.972\tvalid_1's r2: 0.925135\n",
      "[6000]\ttraining's rmse: 505.811\ttraining's r2: 0.961434\tvalid_1's rmse: 702.362\tvalid_1's r2: 0.925265\n",
      "[6200]\ttraining's rmse: 501.566\ttraining's r2: 0.962078\tvalid_1's rmse: 701.734\tvalid_1's r2: 0.925399\n",
      "[6400]\ttraining's rmse: 497.441\ttraining's r2: 0.962699\tvalid_1's rmse: 701.26\tvalid_1's r2: 0.9255\n",
      "[6600]\ttraining's rmse: 493.413\ttraining's r2: 0.963301\tvalid_1's rmse: 700.812\tvalid_1's r2: 0.925595\n",
      "[6800]\ttraining's rmse: 489.51\ttraining's r2: 0.963879\tvalid_1's rmse: 700.294\tvalid_1's r2: 0.925705\n",
      "[7000]\ttraining's rmse: 485.696\ttraining's r2: 0.96444\tvalid_1's rmse: 699.894\tvalid_1's r2: 0.925789\n",
      "[7200]\ttraining's rmse: 482.008\ttraining's r2: 0.964978\tvalid_1's rmse: 699.578\tvalid_1's r2: 0.925856\n",
      "[7400]\ttraining's rmse: 478.346\ttraining's r2: 0.965508\tvalid_1's rmse: 699.078\tvalid_1's r2: 0.925962\n",
      "[7600]\ttraining's rmse: 474.746\ttraining's r2: 0.966025\tvalid_1's rmse: 698.541\tvalid_1's r2: 0.926076\n",
      "[7800]\ttraining's rmse: 471.234\ttraining's r2: 0.966526\tvalid_1's rmse: 698.156\tvalid_1's r2: 0.926158\n",
      "[8000]\ttraining's rmse: 467.722\ttraining's r2: 0.967023\tvalid_1's rmse: 697.875\tvalid_1's r2: 0.926217\n",
      "[8200]\ttraining's rmse: 464.254\ttraining's r2: 0.96751\tvalid_1's rmse: 697.638\tvalid_1's r2: 0.926267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8400]\ttraining's rmse: 460.986\ttraining's r2: 0.967966\tvalid_1's rmse: 697.404\tvalid_1's r2: 0.926317\n",
      "[8600]\ttraining's rmse: 457.718\ttraining's r2: 0.968419\tvalid_1's rmse: 697.187\tvalid_1's r2: 0.926362\n",
      "[8800]\ttraining's rmse: 454.542\ttraining's r2: 0.968856\tvalid_1's rmse: 697.021\tvalid_1's r2: 0.926397\n",
      "[9000]\ttraining's rmse: 451.352\ttraining's r2: 0.969291\tvalid_1's rmse: 696.683\tvalid_1's r2: 0.926469\n",
      "[9200]\ttraining's rmse: 448.245\ttraining's r2: 0.969712\tvalid_1's rmse: 696.519\tvalid_1's r2: 0.926503\n",
      "[9400]\ttraining's rmse: 445.287\ttraining's r2: 0.970111\tvalid_1's rmse: 696.22\tvalid_1's r2: 0.926567\n",
      "[9600]\ttraining's rmse: 442.306\ttraining's r2: 0.97051\tvalid_1's rmse: 696.084\tvalid_1's r2: 0.926595\n",
      "[9800]\ttraining's rmse: 439.315\ttraining's r2: 0.970907\tvalid_1's rmse: 695.925\tvalid_1's r2: 0.926629\n",
      "[10000]\ttraining's rmse: 436.415\ttraining's r2: 0.97129\tvalid_1's rmse: 695.845\tvalid_1's r2: 0.926646\n",
      "[10200]\ttraining's rmse: 433.628\ttraining's r2: 0.971656\tvalid_1's rmse: 695.587\tvalid_1's r2: 0.9267\n",
      "[10400]\ttraining's rmse: 430.823\ttraining's r2: 0.972021\tvalid_1's rmse: 695.627\tvalid_1's r2: 0.926692\n",
      "[10600]\ttraining's rmse: 428.077\ttraining's r2: 0.972377\tvalid_1's rmse: 695.493\tvalid_1's r2: 0.92672\n",
      "[10800]\ttraining's rmse: 425.293\ttraining's r2: 0.972735\tvalid_1's rmse: 695.435\tvalid_1's r2: 0.926732\n",
      "[11000]\ttraining's rmse: 422.616\ttraining's r2: 0.973077\tvalid_1's rmse: 695.436\tvalid_1's r2: 0.926732\n",
      "Early stopping, best iteration is:\n",
      "[10908]\ttraining's rmse: 423.822\ttraining's r2: 0.972923\tvalid_1's rmse: 695.382\tvalid_1's r2: 0.926743\n",
      "Predict 1/2\n",
      "LGB cv score 2: LOSS 0.926743298400512 \n",
      "################################################################################\n",
      "LGB | FOLD 3/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1017.88\ttraining's r2: 0.842853\tvalid_1's rmse: 1045.33\tvalid_1's r2: 0.838523\n",
      "[400]\ttraining's rmse: 822.749\ttraining's r2: 0.897329\tvalid_1's rmse: 852.34\tvalid_1's r2: 0.892643\n",
      "[600]\ttraining's rmse: 762.612\ttraining's r2: 0.911789\tvalid_1's rmse: 802.87\tvalid_1's r2: 0.904744\n",
      "[800]\ttraining's rmse: 727.626\ttraining's r2: 0.919697\tvalid_1's rmse: 779.434\tvalid_1's r2: 0.910223\n",
      "[1000]\ttraining's rmse: 701.558\ttraining's r2: 0.925348\tvalid_1's rmse: 764.349\tvalid_1's r2: 0.913665\n",
      "[1200]\ttraining's rmse: 681.557\ttraining's r2: 0.929544\tvalid_1's rmse: 754.837\tvalid_1's r2: 0.9158\n",
      "[1400]\ttraining's rmse: 664.803\ttraining's r2: 0.932965\tvalid_1's rmse: 746.807\tvalid_1's r2: 0.917582\n",
      "[1600]\ttraining's rmse: 650.7\ttraining's r2: 0.935779\tvalid_1's rmse: 740.363\tvalid_1's r2: 0.918999\n",
      "[1800]\ttraining's rmse: 638.311\ttraining's r2: 0.938201\tvalid_1's rmse: 735.322\tvalid_1's r2: 0.920098\n",
      "[2000]\ttraining's rmse: 627.237\ttraining's r2: 0.940327\tvalid_1's rmse: 731.541\tvalid_1's r2: 0.920917\n",
      "[2200]\ttraining's rmse: 617.201\ttraining's r2: 0.942221\tvalid_1's rmse: 728.531\tvalid_1's r2: 0.921567\n",
      "[2400]\ttraining's rmse: 607.83\ttraining's r2: 0.943963\tvalid_1's rmse: 725.408\tvalid_1's r2: 0.922238\n",
      "[2600]\ttraining's rmse: 599.108\ttraining's r2: 0.945559\tvalid_1's rmse: 723.386\tvalid_1's r2: 0.922671\n",
      "[2800]\ttraining's rmse: 591.094\ttraining's r2: 0.947006\tvalid_1's rmse: 721.438\tvalid_1's r2: 0.923087\n",
      "[3000]\ttraining's rmse: 583.509\ttraining's r2: 0.948357\tvalid_1's rmse: 719.574\tvalid_1's r2: 0.923484\n",
      "[3200]\ttraining's rmse: 576.184\ttraining's r2: 0.949646\tvalid_1's rmse: 718.05\tvalid_1's r2: 0.923807\n",
      "[3400]\ttraining's rmse: 569.214\ttraining's r2: 0.950857\tvalid_1's rmse: 716.834\tvalid_1's r2: 0.924065\n",
      "[3600]\ttraining's rmse: 562.683\ttraining's r2: 0.951978\tvalid_1's rmse: 715.58\tvalid_1's r2: 0.924331\n",
      "[3800]\ttraining's rmse: 556.496\ttraining's r2: 0.953028\tvalid_1's rmse: 714.226\tvalid_1's r2: 0.924617\n",
      "[4000]\ttraining's rmse: 550.526\ttraining's r2: 0.95403\tvalid_1's rmse: 713.001\tvalid_1's r2: 0.924875\n",
      "[4200]\ttraining's rmse: 544.89\ttraining's r2: 0.954967\tvalid_1's rmse: 712.146\tvalid_1's r2: 0.925055\n",
      "[4400]\ttraining's rmse: 539.218\ttraining's r2: 0.955899\tvalid_1's rmse: 711.019\tvalid_1's r2: 0.925292\n",
      "[4600]\ttraining's rmse: 533.817\ttraining's r2: 0.956779\tvalid_1's rmse: 709.964\tvalid_1's r2: 0.925514\n",
      "[4800]\ttraining's rmse: 528.646\ttraining's r2: 0.957612\tvalid_1's rmse: 708.83\tvalid_1's r2: 0.925751\n",
      "[5000]\ttraining's rmse: 523.638\ttraining's r2: 0.958411\tvalid_1's rmse: 707.947\tvalid_1's r2: 0.925936\n",
      "[5200]\ttraining's rmse: 518.899\ttraining's r2: 0.959161\tvalid_1's rmse: 707.317\tvalid_1's r2: 0.926068\n",
      "[5400]\ttraining's rmse: 514.252\ttraining's r2: 0.959889\tvalid_1's rmse: 706.676\tvalid_1's r2: 0.926202\n",
      "[5600]\ttraining's rmse: 509.665\ttraining's r2: 0.960601\tvalid_1's rmse: 706.347\tvalid_1's r2: 0.926271\n",
      "[5800]\ttraining's rmse: 505.298\ttraining's r2: 0.961273\tvalid_1's rmse: 705.788\tvalid_1's r2: 0.926387\n",
      "[6000]\ttraining's rmse: 500.924\ttraining's r2: 0.961941\tvalid_1's rmse: 705.233\tvalid_1's r2: 0.926503\n",
      "[6200]\ttraining's rmse: 496.825\ttraining's r2: 0.962561\tvalid_1's rmse: 704.612\tvalid_1's r2: 0.926632\n",
      "[6400]\ttraining's rmse: 492.788\ttraining's r2: 0.963167\tvalid_1's rmse: 704.086\tvalid_1's r2: 0.926742\n",
      "[6600]\ttraining's rmse: 488.83\ttraining's r2: 0.963757\tvalid_1's rmse: 703.555\tvalid_1's r2: 0.926852\n",
      "[6800]\ttraining's rmse: 484.986\ttraining's r2: 0.964324\tvalid_1's rmse: 703.041\tvalid_1's r2: 0.926959\n",
      "[7000]\ttraining's rmse: 481.19\ttraining's r2: 0.964881\tvalid_1's rmse: 702.598\tvalid_1's r2: 0.927051\n",
      "[7200]\ttraining's rmse: 477.433\ttraining's r2: 0.965427\tvalid_1's rmse: 702.251\tvalid_1's r2: 0.927123\n",
      "[7400]\ttraining's rmse: 473.904\ttraining's r2: 0.965936\tvalid_1's rmse: 701.956\tvalid_1's r2: 0.927185\n",
      "[7600]\ttraining's rmse: 470.327\ttraining's r2: 0.966448\tvalid_1's rmse: 701.622\tvalid_1's r2: 0.927254\n",
      "[7800]\ttraining's rmse: 466.942\ttraining's r2: 0.96693\tvalid_1's rmse: 701.423\tvalid_1's r2: 0.927295\n",
      "[8000]\ttraining's rmse: 463.542\ttraining's r2: 0.967409\tvalid_1's rmse: 701.159\tvalid_1's r2: 0.92735\n",
      "[8200]\ttraining's rmse: 460.145\ttraining's r2: 0.967885\tvalid_1's rmse: 700.917\tvalid_1's r2: 0.9274\n",
      "[8400]\ttraining's rmse: 456.888\ttraining's r2: 0.968338\tvalid_1's rmse: 700.537\tvalid_1's r2: 0.927479\n",
      "[8600]\ttraining's rmse: 453.588\ttraining's r2: 0.968794\tvalid_1's rmse: 700.138\tvalid_1's r2: 0.927561\n",
      "[8800]\ttraining's rmse: 450.442\ttraining's r2: 0.969225\tvalid_1's rmse: 699.935\tvalid_1's r2: 0.927603\n",
      "[9000]\ttraining's rmse: 447.371\ttraining's r2: 0.969644\tvalid_1's rmse: 699.766\tvalid_1's r2: 0.927638\n",
      "[9200]\ttraining's rmse: 444.313\ttraining's r2: 0.970057\tvalid_1's rmse: 699.609\tvalid_1's r2: 0.927671\n",
      "[9400]\ttraining's rmse: 441.23\ttraining's r2: 0.970471\tvalid_1's rmse: 699.387\tvalid_1's r2: 0.927717\n",
      "[9600]\ttraining's rmse: 438.299\ttraining's r2: 0.970862\tvalid_1's rmse: 699.125\tvalid_1's r2: 0.927771\n",
      "[9800]\ttraining's rmse: 435.343\ttraining's r2: 0.971254\tvalid_1's rmse: 699.058\tvalid_1's r2: 0.927784\n",
      "[10000]\ttraining's rmse: 432.477\ttraining's r2: 0.971631\tvalid_1's rmse: 698.842\tvalid_1's r2: 0.927829\n",
      "[10200]\ttraining's rmse: 429.643\ttraining's r2: 0.972002\tvalid_1's rmse: 698.728\tvalid_1's r2: 0.927853\n",
      "[10400]\ttraining's rmse: 426.846\ttraining's r2: 0.972365\tvalid_1's rmse: 698.615\tvalid_1's r2: 0.927876\n",
      "[10600]\ttraining's rmse: 424.127\ttraining's r2: 0.972716\tvalid_1's rmse: 698.423\tvalid_1's r2: 0.927916\n",
      "[10800]\ttraining's rmse: 421.448\ttraining's r2: 0.97306\tvalid_1's rmse: 698.379\tvalid_1's r2: 0.927925\n",
      "[11000]\ttraining's rmse: 418.785\ttraining's r2: 0.973399\tvalid_1's rmse: 698.182\tvalid_1's r2: 0.927965\n",
      "[11200]\ttraining's rmse: 416.305\ttraining's r2: 0.973713\tvalid_1's rmse: 698.106\tvalid_1's r2: 0.927981\n",
      "[11400]\ttraining's rmse: 413.734\ttraining's r2: 0.974037\tvalid_1's rmse: 697.983\tvalid_1's r2: 0.928006\n",
      "[11600]\ttraining's rmse: 411.179\ttraining's r2: 0.974357\tvalid_1's rmse: 697.829\tvalid_1's r2: 0.928038\n",
      "[11800]\ttraining's rmse: 408.685\ttraining's r2: 0.974667\tvalid_1's rmse: 697.774\tvalid_1's r2: 0.92805\n",
      "[12000]\ttraining's rmse: 406.251\ttraining's r2: 0.974968\tvalid_1's rmse: 697.677\tvalid_1's r2: 0.928069\n",
      "[12200]\ttraining's rmse: 403.764\ttraining's r2: 0.975273\tvalid_1's rmse: 697.682\tvalid_1's r2: 0.928069\n",
      "Early stopping, best iteration is:\n",
      "[12080]\ttraining's rmse: 405.262\ttraining's r2: 0.975089\tvalid_1's rmse: 697.617\tvalid_1's r2: 0.928082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict 1/2\n",
      "LGB cv score 3: LOSS 0.9280818695180625 \n",
      "################################################################################\n",
      "LGB | FOLD 4/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1021.98\ttraining's r2: 0.84369\tvalid_1's rmse: 1009.71\tvalid_1's r2: 0.840943\n",
      "[400]\ttraining's rmse: 828.349\ttraining's r2: 0.897311\tvalid_1's rmse: 832.178\tvalid_1's r2: 0.891958\n",
      "[600]\ttraining's rmse: 767.961\ttraining's r2: 0.911738\tvalid_1's rmse: 781.899\tvalid_1's r2: 0.904619\n",
      "[800]\ttraining's rmse: 731.956\ttraining's r2: 0.91982\tvalid_1's rmse: 757.784\tvalid_1's r2: 0.910412\n",
      "[1000]\ttraining's rmse: 706.332\ttraining's r2: 0.925335\tvalid_1's rmse: 743.392\tvalid_1's r2: 0.913782\n",
      "[1200]\ttraining's rmse: 686.083\ttraining's r2: 0.929555\tvalid_1's rmse: 733.381\tvalid_1's r2: 0.916089\n",
      "[1400]\ttraining's rmse: 669.62\ttraining's r2: 0.932895\tvalid_1's rmse: 726.908\tvalid_1's r2: 0.917564\n",
      "[1600]\ttraining's rmse: 655.382\ttraining's r2: 0.935718\tvalid_1's rmse: 721.947\tvalid_1's r2: 0.918685\n",
      "[1800]\ttraining's rmse: 642.638\ttraining's r2: 0.938194\tvalid_1's rmse: 717.485\tvalid_1's r2: 0.919687\n",
      "[2000]\ttraining's rmse: 631.544\ttraining's r2: 0.940309\tvalid_1's rmse: 714.085\tvalid_1's r2: 0.920446\n",
      "[2200]\ttraining's rmse: 621.225\ttraining's r2: 0.942244\tvalid_1's rmse: 711.541\tvalid_1's r2: 0.921012\n",
      "[2400]\ttraining's rmse: 611.717\ttraining's r2: 0.943999\tvalid_1's rmse: 709.069\tvalid_1's r2: 0.92156\n",
      "[2600]\ttraining's rmse: 602.902\ttraining's r2: 0.945601\tvalid_1's rmse: 706.871\tvalid_1's r2: 0.922046\n",
      "[2800]\ttraining's rmse: 594.94\ttraining's r2: 0.947028\tvalid_1's rmse: 705.323\tvalid_1's r2: 0.922387\n",
      "[3000]\ttraining's rmse: 587.158\ttraining's r2: 0.948405\tvalid_1's rmse: 703.916\tvalid_1's r2: 0.922696\n",
      "[3200]\ttraining's rmse: 579.964\ttraining's r2: 0.949662\tvalid_1's rmse: 702.36\tvalid_1's r2: 0.923037\n",
      "[3400]\ttraining's rmse: 573.052\ttraining's r2: 0.950854\tvalid_1's rmse: 700.931\tvalid_1's r2: 0.92335\n",
      "[3600]\ttraining's rmse: 566.321\ttraining's r2: 0.952002\tvalid_1's rmse: 699.713\tvalid_1's r2: 0.923616\n",
      "[3800]\ttraining's rmse: 560.269\ttraining's r2: 0.953022\tvalid_1's rmse: 698.799\tvalid_1's r2: 0.923816\n",
      "[4000]\ttraining's rmse: 554.329\ttraining's r2: 0.954013\tvalid_1's rmse: 697.782\tvalid_1's r2: 0.924038\n",
      "[4200]\ttraining's rmse: 548.522\ttraining's r2: 0.954972\tvalid_1's rmse: 696.874\tvalid_1's r2: 0.924235\n",
      "[4400]\ttraining's rmse: 542.858\ttraining's r2: 0.955897\tvalid_1's rmse: 696.19\tvalid_1's r2: 0.924384\n",
      "[4600]\ttraining's rmse: 537.148\ttraining's r2: 0.95682\tvalid_1's rmse: 695.602\tvalid_1's r2: 0.924511\n",
      "[4800]\ttraining's rmse: 531.945\ttraining's r2: 0.957652\tvalid_1's rmse: 694.896\tvalid_1's r2: 0.924665\n",
      "[5000]\ttraining's rmse: 526.919\ttraining's r2: 0.958449\tvalid_1's rmse: 694.209\tvalid_1's r2: 0.924813\n",
      "[5200]\ttraining's rmse: 522.078\ttraining's r2: 0.959208\tvalid_1's rmse: 693.714\tvalid_1's r2: 0.924921\n",
      "[5400]\ttraining's rmse: 517.319\ttraining's r2: 0.959949\tvalid_1's rmse: 693.275\tvalid_1's r2: 0.925016\n",
      "[5600]\ttraining's rmse: 512.721\ttraining's r2: 0.960658\tvalid_1's rmse: 692.66\tvalid_1's r2: 0.925149\n",
      "[5800]\ttraining's rmse: 508.254\ttraining's r2: 0.96134\tvalid_1's rmse: 692.231\tvalid_1's r2: 0.925241\n",
      "[6000]\ttraining's rmse: 503.903\ttraining's r2: 0.961999\tvalid_1's rmse: 691.809\tvalid_1's r2: 0.925332\n",
      "[6200]\ttraining's rmse: 499.699\ttraining's r2: 0.962631\tvalid_1's rmse: 691.676\tvalid_1's r2: 0.925361\n",
      "[6400]\ttraining's rmse: 495.583\ttraining's r2: 0.963244\tvalid_1's rmse: 691.295\tvalid_1's r2: 0.925443\n",
      "[6600]\ttraining's rmse: 491.586\ttraining's r2: 0.963834\tvalid_1's rmse: 690.957\tvalid_1's r2: 0.925516\n",
      "[6800]\ttraining's rmse: 487.673\ttraining's r2: 0.964408\tvalid_1's rmse: 690.715\tvalid_1's r2: 0.925568\n",
      "[7000]\ttraining's rmse: 483.956\ttraining's r2: 0.964948\tvalid_1's rmse: 690.652\tvalid_1's r2: 0.925582\n",
      "[7200]\ttraining's rmse: 480.248\ttraining's r2: 0.965483\tvalid_1's rmse: 690.473\tvalid_1's r2: 0.92562\n",
      "[7400]\ttraining's rmse: 476.617\ttraining's r2: 0.966003\tvalid_1's rmse: 690.172\tvalid_1's r2: 0.925685\n",
      "[7600]\ttraining's rmse: 473.02\ttraining's r2: 0.966514\tvalid_1's rmse: 689.989\tvalid_1's r2: 0.925725\n",
      "[7800]\ttraining's rmse: 469.544\ttraining's r2: 0.967005\tvalid_1's rmse: 689.862\tvalid_1's r2: 0.925752\n",
      "[8000]\ttraining's rmse: 466.143\ttraining's r2: 0.967481\tvalid_1's rmse: 689.633\tvalid_1's r2: 0.925801\n",
      "[8200]\ttraining's rmse: 462.676\ttraining's r2: 0.967963\tvalid_1's rmse: 689.403\tvalid_1's r2: 0.925851\n",
      "[8400]\ttraining's rmse: 459.373\ttraining's r2: 0.968419\tvalid_1's rmse: 689.197\tvalid_1's r2: 0.925895\n",
      "[8600]\ttraining's rmse: 456.186\ttraining's r2: 0.968855\tvalid_1's rmse: 689.044\tvalid_1's r2: 0.925928\n",
      "[8800]\ttraining's rmse: 452.995\ttraining's r2: 0.96929\tvalid_1's rmse: 688.746\tvalid_1's r2: 0.925992\n",
      "[9000]\ttraining's rmse: 449.895\ttraining's r2: 0.969709\tvalid_1's rmse: 688.58\tvalid_1's r2: 0.926028\n",
      "[9200]\ttraining's rmse: 446.877\ttraining's r2: 0.970114\tvalid_1's rmse: 688.35\tvalid_1's r2: 0.926077\n",
      "[9400]\ttraining's rmse: 443.861\ttraining's r2: 0.970516\tvalid_1's rmse: 688.325\tvalid_1's r2: 0.926083\n",
      "[9600]\ttraining's rmse: 440.88\ttraining's r2: 0.97091\tvalid_1's rmse: 688.03\tvalid_1's r2: 0.926146\n",
      "[9800]\ttraining's rmse: 437.891\ttraining's r2: 0.971303\tvalid_1's rmse: 687.958\tvalid_1's r2: 0.926161\n",
      "[10000]\ttraining's rmse: 435.006\ttraining's r2: 0.97168\tvalid_1's rmse: 687.87\tvalid_1's r2: 0.92618\n",
      "[10200]\ttraining's rmse: 432.124\ttraining's r2: 0.972054\tvalid_1's rmse: 687.793\tvalid_1's r2: 0.926197\n",
      "[10400]\ttraining's rmse: 429.348\ttraining's r2: 0.972412\tvalid_1's rmse: 687.782\tvalid_1's r2: 0.926199\n",
      "[10600]\ttraining's rmse: 426.637\ttraining's r2: 0.97276\tvalid_1's rmse: 687.761\tvalid_1's r2: 0.926204\n",
      "Early stopping, best iteration is:\n",
      "[10435]\ttraining's rmse: 428.884\ttraining's r2: 0.972472\tvalid_1's rmse: 687.693\tvalid_1's r2: 0.926218\n",
      "Predict 1/2\n",
      "LGB cv score 4: LOSS 0.9262181723623553 \n",
      "################################################################################\n",
      "LGB | FOLD 5/5\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[200]\ttraining's rmse: 1018.12\ttraining's r2: 0.844507\tvalid_1's rmse: 1034.31\tvalid_1's r2: 0.834744\n",
      "[400]\ttraining's rmse: 820.94\ttraining's r2: 0.898903\tvalid_1's rmse: 853.174\tvalid_1's r2: 0.887557\n",
      "[600]\ttraining's rmse: 762.329\ttraining's r2: 0.912823\tvalid_1's rmse: 803.47\tvalid_1's r2: 0.900277\n",
      "[800]\ttraining's rmse: 726.839\ttraining's r2: 0.920751\tvalid_1's rmse: 776.932\tvalid_1's r2: 0.906755\n",
      "[1000]\ttraining's rmse: 701.525\ttraining's r2: 0.926175\tvalid_1's rmse: 760.966\tvalid_1's r2: 0.910548\n",
      "[1200]\ttraining's rmse: 681.758\ttraining's r2: 0.930277\tvalid_1's rmse: 750.275\tvalid_1's r2: 0.913044\n",
      "[1400]\ttraining's rmse: 665.455\ttraining's r2: 0.933571\tvalid_1's rmse: 742.444\tvalid_1's r2: 0.91485\n",
      "[1600]\ttraining's rmse: 651.623\ttraining's r2: 0.936304\tvalid_1's rmse: 736.231\tvalid_1's r2: 0.916269\n",
      "[1800]\ttraining's rmse: 639.556\ttraining's r2: 0.938641\tvalid_1's rmse: 731.36\tvalid_1's r2: 0.917374\n",
      "[2000]\ttraining's rmse: 628.583\ttraining's r2: 0.940729\tvalid_1's rmse: 727.603\tvalid_1's r2: 0.91822\n",
      "[2200]\ttraining's rmse: 618.883\ttraining's r2: 0.942544\tvalid_1's rmse: 724.312\tvalid_1's r2: 0.918958\n",
      "[2400]\ttraining's rmse: 609.994\ttraining's r2: 0.944183\tvalid_1's rmse: 721.326\tvalid_1's r2: 0.919625\n",
      "[2600]\ttraining's rmse: 601.691\ttraining's r2: 0.945692\tvalid_1's rmse: 718.664\tvalid_1's r2: 0.920217\n",
      "[2800]\ttraining's rmse: 593.97\ttraining's r2: 0.947077\tvalid_1's rmse: 716.592\tvalid_1's r2: 0.920677\n",
      "[3000]\ttraining's rmse: 586.556\ttraining's r2: 0.94839\tvalid_1's rmse: 715.072\tvalid_1's r2: 0.921013\n",
      "[3200]\ttraining's rmse: 579.661\ttraining's r2: 0.949596\tvalid_1's rmse: 713.322\tvalid_1's r2: 0.921399\n",
      "[3400]\ttraining's rmse: 573.064\ttraining's r2: 0.950737\tvalid_1's rmse: 711.974\tvalid_1's r2: 0.921696\n",
      "[3600]\ttraining's rmse: 566.977\ttraining's r2: 0.951778\tvalid_1's rmse: 710.691\tvalid_1's r2: 0.921978\n",
      "[3800]\ttraining's rmse: 560.825\ttraining's r2: 0.952818\tvalid_1's rmse: 709.347\tvalid_1's r2: 0.922272\n",
      "[4000]\ttraining's rmse: 555.014\ttraining's r2: 0.953791\tvalid_1's rmse: 708.407\tvalid_1's r2: 0.922478\n",
      "[4200]\ttraining's rmse: 549.472\ttraining's r2: 0.954709\tvalid_1's rmse: 707.628\tvalid_1's r2: 0.922649\n",
      "[4400]\ttraining's rmse: 543.933\ttraining's r2: 0.955618\tvalid_1's rmse: 706.588\tvalid_1's r2: 0.922876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4600]\ttraining's rmse: 538.601\ttraining's r2: 0.956484\tvalid_1's rmse: 705.669\tvalid_1's r2: 0.923076\n",
      "[4800]\ttraining's rmse: 533.628\ttraining's r2: 0.957284\tvalid_1's rmse: 704.875\tvalid_1's r2: 0.923249\n",
      "[5000]\ttraining's rmse: 528.627\ttraining's r2: 0.958081\tvalid_1's rmse: 703.98\tvalid_1's r2: 0.923444\n",
      "[5200]\ttraining's rmse: 523.85\ttraining's r2: 0.958835\tvalid_1's rmse: 703.526\tvalid_1's r2: 0.923543\n",
      "[5400]\ttraining's rmse: 519.297\ttraining's r2: 0.959547\tvalid_1's rmse: 702.889\tvalid_1's r2: 0.923681\n",
      "[5600]\ttraining's rmse: 514.74\ttraining's r2: 0.960254\tvalid_1's rmse: 702.493\tvalid_1's r2: 0.923767\n",
      "[5800]\ttraining's rmse: 510.403\ttraining's r2: 0.960921\tvalid_1's rmse: 701.976\tvalid_1's r2: 0.923879\n",
      "[6000]\ttraining's rmse: 506.098\ttraining's r2: 0.961577\tvalid_1's rmse: 701.61\tvalid_1's r2: 0.923959\n",
      "[6200]\ttraining's rmse: 501.888\ttraining's r2: 0.962214\tvalid_1's rmse: 701.179\tvalid_1's r2: 0.924052\n",
      "[6400]\ttraining's rmse: 497.774\ttraining's r2: 0.962831\tvalid_1's rmse: 700.699\tvalid_1's r2: 0.924156\n",
      "[6600]\ttraining's rmse: 493.663\ttraining's r2: 0.963442\tvalid_1's rmse: 700.302\tvalid_1's r2: 0.924242\n",
      "[6800]\ttraining's rmse: 489.847\ttraining's r2: 0.964005\tvalid_1's rmse: 699.744\tvalid_1's r2: 0.924363\n",
      "[7000]\ttraining's rmse: 486.082\ttraining's r2: 0.964556\tvalid_1's rmse: 699.305\tvalid_1's r2: 0.924458\n",
      "[7200]\ttraining's rmse: 482.379\ttraining's r2: 0.965094\tvalid_1's rmse: 699.098\tvalid_1's r2: 0.924502\n",
      "[7400]\ttraining's rmse: 478.628\ttraining's r2: 0.965635\tvalid_1's rmse: 698.813\tvalid_1's r2: 0.924564\n",
      "[7600]\ttraining's rmse: 475.049\ttraining's r2: 0.966147\tvalid_1's rmse: 698.497\tvalid_1's r2: 0.924632\n",
      "[7800]\ttraining's rmse: 471.544\ttraining's r2: 0.966645\tvalid_1's rmse: 698.279\tvalid_1's r2: 0.924679\n",
      "[8000]\ttraining's rmse: 468.099\ttraining's r2: 0.967131\tvalid_1's rmse: 698.052\tvalid_1's r2: 0.924728\n",
      "[8200]\ttraining's rmse: 464.805\ttraining's r2: 0.967591\tvalid_1's rmse: 697.841\tvalid_1's r2: 0.924774\n",
      "[8400]\ttraining's rmse: 461.471\ttraining's r2: 0.968055\tvalid_1's rmse: 697.593\tvalid_1's r2: 0.924827\n",
      "[8600]\ttraining's rmse: 458.181\ttraining's r2: 0.968509\tvalid_1's rmse: 697.381\tvalid_1's r2: 0.924873\n",
      "[8800]\ttraining's rmse: 455.043\ttraining's r2: 0.968939\tvalid_1's rmse: 697.158\tvalid_1's r2: 0.924921\n",
      "[9000]\ttraining's rmse: 451.94\ttraining's r2: 0.969361\tvalid_1's rmse: 696.942\tvalid_1's r2: 0.924967\n",
      "[9200]\ttraining's rmse: 448.761\ttraining's r2: 0.96979\tvalid_1's rmse: 696.808\tvalid_1's r2: 0.924996\n",
      "[9400]\ttraining's rmse: 445.747\ttraining's r2: 0.970195\tvalid_1's rmse: 696.648\tvalid_1's r2: 0.925031\n",
      "[9600]\ttraining's rmse: 442.775\ttraining's r2: 0.970591\tvalid_1's rmse: 696.462\tvalid_1's r2: 0.925071\n",
      "[9800]\ttraining's rmse: 439.886\ttraining's r2: 0.970973\tvalid_1's rmse: 696.151\tvalid_1's r2: 0.925138\n",
      "[10000]\ttraining's rmse: 437.015\ttraining's r2: 0.971351\tvalid_1's rmse: 696.009\tvalid_1's r2: 0.925168\n",
      "[10200]\ttraining's rmse: 434.162\ttraining's r2: 0.971724\tvalid_1's rmse: 695.91\tvalid_1's r2: 0.925189\n",
      "[10400]\ttraining's rmse: 431.386\ttraining's r2: 0.972084\tvalid_1's rmse: 695.747\tvalid_1's r2: 0.925224\n",
      "[10600]\ttraining's rmse: 428.692\ttraining's r2: 0.972432\tvalid_1's rmse: 695.626\tvalid_1's r2: 0.92525\n",
      "[10800]\ttraining's rmse: 426.071\ttraining's r2: 0.972768\tvalid_1's rmse: 695.527\tvalid_1's r2: 0.925272\n",
      "[11000]\ttraining's rmse: 423.418\ttraining's r2: 0.973106\tvalid_1's rmse: 695.443\tvalid_1's r2: 0.92529\n",
      "[11200]\ttraining's rmse: 420.84\ttraining's r2: 0.973432\tvalid_1's rmse: 695.198\tvalid_1's r2: 0.925342\n",
      "[11400]\ttraining's rmse: 418.234\ttraining's r2: 0.973761\tvalid_1's rmse: 695.142\tvalid_1's r2: 0.925354\n",
      "[11600]\ttraining's rmse: 415.64\ttraining's r2: 0.974085\tvalid_1's rmse: 695.024\tvalid_1's r2: 0.92538\n",
      "[11800]\ttraining's rmse: 413.08\ttraining's r2: 0.974403\tvalid_1's rmse: 694.898\tvalid_1's r2: 0.925407\n",
      "[12000]\ttraining's rmse: 410.611\ttraining's r2: 0.974708\tvalid_1's rmse: 694.837\tvalid_1's r2: 0.92542\n",
      "[12200]\ttraining's rmse: 408.079\ttraining's r2: 0.975019\tvalid_1's rmse: 694.675\tvalid_1's r2: 0.925455\n",
      "[12400]\ttraining's rmse: 405.647\ttraining's r2: 0.975316\tvalid_1's rmse: 694.562\tvalid_1's r2: 0.925479\n",
      "[12600]\ttraining's rmse: 403.235\ttraining's r2: 0.975609\tvalid_1's rmse: 694.548\tvalid_1's r2: 0.925482\n",
      "[12800]\ttraining's rmse: 400.827\ttraining's r2: 0.975899\tvalid_1's rmse: 694.389\tvalid_1's r2: 0.925516\n",
      "[13000]\ttraining's rmse: 398.472\ttraining's r2: 0.976182\tvalid_1's rmse: 694.34\tvalid_1's r2: 0.925526\n",
      "[13200]\ttraining's rmse: 396.131\ttraining's r2: 0.976461\tvalid_1's rmse: 694.294\tvalid_1's r2: 0.925536\n",
      "Early stopping, best iteration is:\n",
      "[13149]\ttraining's rmse: 396.739\ttraining's r2: 0.976388\tvalid_1's rmse: 694.273\tvalid_1's r2: 0.925541\n",
      "Predict 1/2\n",
      "LGB cv score 5: LOSS 0.9255408849061617 \n",
      "################################################################################\n",
      "LGB cv mean LOSS score : 0.9252013903058343\n",
      "LGB cv std LOSS score : 0.0030072146782237565\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "def run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n",
    "    folds=5\n",
    "    \n",
    "    kf = KFold(n_splits=folds, random_state=1017, shuffle=True)\n",
    "    fold_splits = kf.split(train, target)\n",
    "    \n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    \n",
    "    pred_train = np.zeros((train.shape[0], folds))\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    i = 1\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print( label + ' | FOLD ' + str(i) + '/'+str(folds))\n",
    "        if isinstance(train, pd.DataFrame):\n",
    "            dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        else:\n",
    "            dev_X, val_X = train[dev_index], train[val_index]\n",
    "            dev_y, val_y = target[dev_index], target[val_index]\n",
    "        params2 = params.copy()\n",
    "        pred_val_y, pred_test_y, importances = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n",
    "        \n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "#         temp=pd.DataFrame(index=range(len(val_y)))\n",
    "#         temp['y_true']=val_y.values\n",
    "#         temp['area']=val_X['area'].values\n",
    "#         temp['y_true']=temp['y_true']*temp['area']\n",
    "#         temp['y_pred']=pred_val_y\n",
    "#         temp['y_pred']=temp['y_pred']*temp['area']\n",
    "      \n",
    "        if eval_fn is not None:\n",
    "#             cv_score = eval_fn(temp['y_true'], temp['y_pred'])\n",
    "            cv_score = eval_fn(val_y,pred_val_y)\n",
    "            \n",
    "            cv_scores.append(cv_score)\n",
    "          \n",
    "           \n",
    "            print(label + ' cv score {}: LOSS {} '.format(i, cv_score))\n",
    "            \n",
    "            \n",
    "            print(\"##\"*40)\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature'] =train.columns.values\n",
    "        fold_importance_df['importance'] =importances\n",
    "        fold_importance_df['fold'] = i\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n",
    "        i += 1\n",
    "       \n",
    "\n",
    "    print('{} cv mean LOSS score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv std LOSS score : {}'.format(label, np.std(cv_scores)))\n",
    "   \n",
    "\n",
    "    \n",
    "    pred_full_test = pred_full_test / float(folds)\n",
    "    results = {'label': label,\n",
    "               'train': pred_train, 'test': pred_full_test,\n",
    "                'cv': cv_scores, \n",
    "               'importance': feature_importance_df,\n",
    "               }\n",
    "    return results\n",
    "\n",
    "params = {\n",
    "        'bagging_freq': 1,\n",
    "     \"bagging_seed\": 23,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'boost': 'gbdt',\n",
    "        'feature_fraction': 0.4,\n",
    "        'learning_rate': 0.01,\n",
    "        'max_depth': -1,\n",
    "#     \"lambda_l1\": 0.2,\n",
    "#         'metric': 'l2',\n",
    "#         'min_data_in_leaf': 20,\n",
    "#     'min_child_samples':20,\n",
    "      \"metric\": 'rmse',\n",
    "        'num_leaves': 15,\n",
    "        'num_threads': -1,\n",
    "        \n",
    "        'objective': 'regression_l2',\n",
    "        'verbosity': -1,\n",
    "     'verbose_eval': 200,\n",
    "          \"random_state\":1017,\n",
    "          'num_rounds': 50000,\n",
    "     'early_stop': 200,\n",
    "    }\n",
    "\n",
    "def runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "#     print('Prep LGB')\n",
    "    d_train = lgb.Dataset(train_X, label=train_y)\n",
    "    d_valid = lgb.Dataset(test_X, label=test_y)\n",
    "    watchlist = [d_train, d_valid]\n",
    "#     print('Train LGB')\n",
    "    num_rounds = params.pop('num_rounds')\n",
    "    verbose_eval = params.pop('verbose_eval')\n",
    "    early_stop = None\n",
    "    if params.get('early_stop'):\n",
    "        early_stop = params.pop('early_stop')\n",
    "    model = lgb.train(params,\n",
    "                      train_set=d_train,\n",
    "                      num_boost_round=num_rounds,\n",
    "                      valid_sets=watchlist,\n",
    "                      feval=r2,\n",
    "                      verbose_eval=verbose_eval,\n",
    "                      early_stopping_rounds=early_stop)\n",
    "    print('Predict 1/2')\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "   \n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    \n",
    "    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance()\n",
    " \n",
    "results = run_cv_model(train[features], test[features], train[label], runLGB, params, r2_score, 'LGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>area</td>\n",
       "      <td>6204.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>per_room</td>\n",
       "      <td>4824.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>communityName_pred_mean</td>\n",
       "      <td>3687.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>area_mean</td>\n",
       "      <td>3188.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>communityName_pred_max</td>\n",
       "      <td>3014.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>communityName_pred_min</td>\n",
       "      <td>2741.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>houseType</td>\n",
       "      <td>2593.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>day_pred_freq</td>\n",
       "      <td>2528.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>area_skew</td>\n",
       "      <td>2464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>day_pred_std</td>\n",
       "      <td>2457.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>area_std</td>\n",
       "      <td>2436.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>area_min</td>\n",
       "      <td>2428.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>day_pred_mean</td>\n",
       "      <td>2254.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tradeMeanPrice</td>\n",
       "      <td>2227.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>month_skew</td>\n",
       "      <td>2208.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>day_pred_kurt</td>\n",
       "      <td>2189.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>day_skew</td>\n",
       "      <td>2187.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>day_pred_skew</td>\n",
       "      <td>2175.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>area_max</td>\n",
       "      <td>2170.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>day_std</td>\n",
       "      <td>2160.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>month_std</td>\n",
       "      <td>2158.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>communityName_pred_std</td>\n",
       "      <td>2117.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>day_pred_unique</td>\n",
       "      <td>1971.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>plate_pred_mean</td>\n",
       "      <td>1906.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>day_mean</td>\n",
       "      <td>1800.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>plate_pred_std</td>\n",
       "      <td>1785.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>totalFloor_pred_std</td>\n",
       "      <td>1781.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>tradeNewMeanPrice</td>\n",
       "      <td>1777.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>communityName_pred_kurt</td>\n",
       "      <td>1748.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>uv</td>\n",
       "      <td>1747.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>rentType</td>\n",
       "      <td>657.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>bankNum</td>\n",
       "      <td>649.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>day_min</td>\n",
       "      <td>637.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>plate</td>\n",
       "      <td>603.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>buildYear_pred_max</td>\n",
       "      <td>570.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>gymNum</td>\n",
       "      <td>541.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>region</td>\n",
       "      <td>541.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>newWorkers</td>\n",
       "      <td>527.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>houseToward</td>\n",
       "      <td>520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>shopNum</td>\n",
       "      <td>508.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>saleSecHouseNum</td>\n",
       "      <td>507.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>residentPopulation</td>\n",
       "      <td>506.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>drugStoreNum</td>\n",
       "      <td>476.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>superMarketNum</td>\n",
       "      <td>473.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>privateSchoolNum</td>\n",
       "      <td>446.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>month_min</td>\n",
       "      <td>405.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>subwayStationNum</td>\n",
       "      <td>404.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>mallNum</td>\n",
       "      <td>371.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>parkNum</td>\n",
       "      <td>359.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>hospitalNum</td>\n",
       "      <td>345.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>interSchoolNum</td>\n",
       "      <td>341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>supplyNewNum</td>\n",
       "      <td>338.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>month_max</td>\n",
       "      <td>323.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>lookNum</td>\n",
       "      <td>290.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>supplyLandArea</td>\n",
       "      <td>211.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>landMeanPrice</td>\n",
       "      <td>138.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>tradeLandArea</td>\n",
       "      <td>97.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>landTotalPrice</td>\n",
       "      <td>85.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>supplyLandNum</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>tradeLandNum</td>\n",
       "      <td>44.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     feature  importance\n",
       "0                       area      6204.2\n",
       "1                   per_room      4824.6\n",
       "2    communityName_pred_mean      3687.8\n",
       "3                  area_mean      3188.8\n",
       "4     communityName_pred_max      3014.4\n",
       "5     communityName_pred_min      2741.0\n",
       "6                  houseType      2593.0\n",
       "7              day_pred_freq      2528.4\n",
       "8                  area_skew      2464.0\n",
       "9               day_pred_std      2457.8\n",
       "10                  area_std      2436.8\n",
       "11                  area_min      2428.2\n",
       "12             day_pred_mean      2254.8\n",
       "13            tradeMeanPrice      2227.6\n",
       "14                month_skew      2208.4\n",
       "15             day_pred_kurt      2189.6\n",
       "16                  day_skew      2187.6\n",
       "17             day_pred_skew      2175.2\n",
       "18                  area_max      2170.2\n",
       "19                   day_std      2160.8\n",
       "20                 month_std      2158.2\n",
       "21    communityName_pred_std      2117.2\n",
       "22           day_pred_unique      1971.6\n",
       "23           plate_pred_mean      1906.2\n",
       "24                  day_mean      1800.8\n",
       "25            plate_pred_std      1785.4\n",
       "26       totalFloor_pred_std      1781.2\n",
       "27         tradeNewMeanPrice      1777.2\n",
       "28   communityName_pred_kurt      1748.6\n",
       "29                        uv      1747.2\n",
       "..                       ...         ...\n",
       "80                  rentType       657.2\n",
       "81                   bankNum       649.0\n",
       "82                   day_min       637.8\n",
       "83                     plate       603.0\n",
       "84        buildYear_pred_max       570.2\n",
       "85                    gymNum       541.6\n",
       "86                    region       541.6\n",
       "87                newWorkers       527.2\n",
       "88               houseToward       520.0\n",
       "89                   shopNum       508.6\n",
       "90           saleSecHouseNum       507.2\n",
       "91        residentPopulation       506.6\n",
       "92              drugStoreNum       476.6\n",
       "93            superMarketNum       473.4\n",
       "94          privateSchoolNum       446.4\n",
       "95                 month_min       405.6\n",
       "96          subwayStationNum       404.4\n",
       "97                   mallNum       371.4\n",
       "98                   parkNum       359.4\n",
       "99               hospitalNum       345.4\n",
       "100           interSchoolNum       341.0\n",
       "101             supplyNewNum       338.2\n",
       "102                month_max       323.2\n",
       "103                  lookNum       290.4\n",
       "104           supplyLandArea       211.6\n",
       "105            landMeanPrice       138.8\n",
       "106            tradeLandArea        97.8\n",
       "107           landTotalPrice        85.4\n",
       "108            supplyLandNum        72.0\n",
       "109             tradeLandNum        44.4\n",
       "\n",
       "[110 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\n",
    "imp=imports.sort_values('importance', ascending=False)\n",
    "imp.index=range(len(imp))\n",
    "imp.iloc[:310]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=imp.feature.values[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "f15=imp.feature.values[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = [r[0] for r in results['train']]\n",
    "test_predictions = [r[0] for r in results['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\envs\\python35\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sub=test[['ID']]\n",
    "sub[label]=test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub[label]=sub[label]*test['area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[[label]].to_csv(\"submit.csv\",index=False,header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
